# -*- coding: utf-8 -*-
"""Word2VecA-Z.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19zl7EA3vD2hl8SS8hJJz98v_UQH8b1Bt
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import nltk
import re
from wordcloud import WordCloud

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Embedding, LSTM, Conv1D, MaxPool1D
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score

news = pd.read_csv("/content/fake_or_real_news.csv")

news.head()
news.keys()

#Counting by Label
for key,count in news.label.value_counts().iteritems():
    print(f"{key}:\t{count}")
    
#Getting Total Rows
print(f"Total Records:\t{news.shape[0]}")

plt.figure(figsize=(8,5))
sns.countplot("label", data=news)
plt.show()

import nltk
nltk.download('stopwords')

#Word Cloud
texts = ''
for news_i in news.text.values:
    texts += f" {news_i}"
wordcloud = WordCloud(
    width = 3000,
    height = 2000,
    background_color = 'black',
    stopwords = set(nltk.corpus.stopwords.words("english"))).generate(texts)
fig = plt.figure(
    figsize = (40, 30),
    facecolor = 'k',
    edgecolor = 'k')
plt.imshow(wordcloud, interpolation = 'bilinear')
plt.axis('off')
plt.tight_layout(pad=0)
plt.show()
del texts

news.head()

#checking for rows with empty text like row:8970
[index for index,text in enumerate(news.text.values) if str(text).strip() == '']
#seems only one :)

#dropping the records
news = news.drop([106,710,
 806,
 919,
 940,
 1664,
 1736,
 1851,
 1883,
 1941,
 2244,
 2426,
 2576,
 2662,
 2788,
 2832,
 3073,
 3350,
 3511,
 3641,
 3642,
 4014,
 4142,
 4253,
 4713,
 4744,
 5017,
 5088,
 5213,
 5581,
 5639,
 5699,
 5772,
 6064,
 6175,
 6328]
, axis=0)

news.head(6329)

news.keys()

fake = news[news['label'] == 'FAKE']
fake.head()

real = news[news['label'] == 'REAL']
real.head()

# Adding class Information
real["class"] = 1
fake["class"] = 0

#Combining Title and Text
real["text"] = real["title"] + " " + real["text"]
fake["text"] = fake["title"] + " " + fake["text"]

# Subject is diffrent for real and fake thus dropping it
# Aldo dropping Date, title and Publication Info of real
real = real.drop(["Unnamed: 0","title","label"], axis=1)
fake = fake.drop(["Unnamed: 0","title","label"], axis=1)

#Combining both into new dataframe
data = fake
del fake

nltk.download('stopwords')
nltk.download('punkt')

y = data["class"].values
#Converting X to format acceptable by gensim, removing annd punctuation stopwords in the process
X = []
stop_words = set(nltk.corpus.stopwords.words("english"))
tokenizer = nltk.tokenize.RegexpTokenizer(r'\w+')
for par in data["text"].values:
    tmp = []
    sentences = nltk.sent_tokenize(par)
    for sent in sentences:
        sent = sent.lower()
        tokens = tokenizer.tokenize(sent)
        filtered_words = [w.strip() for w in tokens if w not in stop_words and len(w) > 1]
        tmp.extend(filtered_words)
    X.append(tmp)

del data

import gensim

#Dimension of vectors we are generating
EMBEDDING_DIM = 100

#Creating Word Vectors by Word2Vec Method (takes time...)
w2v_model = gensim.models.Word2Vec(sentences=X, size=EMBEDDING_DIM, window=5, min_count=1)

len(w2v_model.wv.vocab)

# Tokenizing Text -> Repsesenting each word by a number
# Mapping of orginal word to number is preserved in word_index property of tokenizer

#Tokenized applies basic processing like changing it yo lower case, explicitely setting that as False
tokenizer = Tokenizer()
tokenizer.fit_on_texts(X)

X = tokenizer.texts_to_sequences(X)

# lets check the first 10 words of first news
#every word has been represented with a number
X[0][:10]

#Lets check few word to numerical replesentation
#Mapping is preserved in dictionary -> word_index property of instance
word_index = tokenizer.word_index
for word, num in word_index.items():
    print(f"{word} -> {num}")
    if num == 10:
        break

# For determining size of input...

# Making histogram for no of words in news shows that most news article are under 700 words.
# Lets keep each news small and truncate all news to 700 while tokenizing
plt.hist([len(x) for x in X], bins=500)
plt.show()

# Its heavily skewed. There are news with 5000 words? Lets truncate these outliers :)

nos = np.array([len(x) for x in X])
len(nos[nos  < 700])

#Lets keep all news to 700, add padding to news with less than 700 words and truncating long ones
maxlen = 700 

#Making all news of size maxlen defined above
X = pad_sequences(X, maxlen=maxlen)

#all news has 700 words (in numerical form now). If they had less words, they have been padded with 0
# 0 is not associated to any word, as mapping of words started from 1
# 0 will also be used later, if unknows word is encountered in test set
len(X[0])

# Adding 1 because of reserved 0 index
# Embedding Layer creates one more vector for "UNKNOWN" words, or padded words (0s). This Vector is filled with zeros.
# Thus our vocab size inceeases by 1
vocab_size = len(tokenizer.word_index) + 1

# Function to create weight matrix from word2vec gensim model
def get_weight_matrix(model, vocab):
    # total vocabulary size plus 0 for unknown words
    vocab_size = len(vocab) + 1
    # define weight matrix dimensions with all 0
    weight_matrix = np.zeros((vocab_size, EMBEDDING_DIM))
    # step vocab, store vectors using the Tokenizer's integer mapping
    for word, i in vocab.items():
        weight_matrix[i] = model[word]
    return weight_matrix

#Getting embedding vectors from word2vec and usings it as weights of non-trainable keras embedding layer
embedding_vectors = get_weight_matrix(w2v_model, word_index)
pd.DataFrame(embedding_vectors).to_csv('/content/fake.csv')

len(embedding_vectors[0])

#Combining both into new dataframe
data = real
del real

nltk.download('stopwords')
nltk.download('punkt')

y = data["class"].values
#Converting X to format acceptable by gensim, removing annd punctuation stopwords in the process
X = []
stop_words = set(nltk.corpus.stopwords.words("english"))
tokenizer = nltk.tokenize.RegexpTokenizer(r'\w+')
for par in data["text"].values:
    tmp = []
    sentences = nltk.sent_tokenize(par)
    for sent in sentences:
        sent = sent.lower()
        tokens = tokenizer.tokenize(sent)
        filtered_words = [w.strip() for w in tokens if w not in stop_words and len(w) > 1]
        tmp.extend(filtered_words)
    X.append(tmp)

del data

import gensim

#Dimension of vectors we are generating
EMBEDDING_DIM = 100

#Creating Word Vectors by Word2Vec Method (takes time...)
w2v_model = gensim.models.Word2Vec(sentences=X, size=EMBEDDING_DIM, window=5, min_count=1)

len(w2v_model.wv.vocab)

# Tokenizing Text -> Repsesenting each word by a number
# Mapping of orginal word to number is preserved in word_index property of tokenizer

#Tokenized applies basic processing like changing it yo lower case, explicitely setting that as False
tokenizer = Tokenizer()
tokenizer.fit_on_texts(X)

X = tokenizer.texts_to_sequences(X)

# lets check the first 10 words of first news
#every word has been represented with a number
X[0][:10]

#Lets check few word to numerical replesentation
#Mapping is preserved in dictionary -> word_index property of instance
word_index = tokenizer.word_index
for word, num in word_index.items():
    print(f"{word} -> {num}")
    if num == 10:
        break

# For determining size of input...

# Making histogram for no of words in news shows that most news article are under 700 words.
# Lets keep each news small and truncate all news to 700 while tokenizing
plt.hist([len(x) for x in X], bins=500)
plt.show()

# Its heavily skewed. There are news with 5000 words? Lets truncate these outliers :)

nos = np.array([len(x) for x in X])
len(nos[nos  < 700])

#Lets keep all news to 700, add padding to news with less than 700 words and truncating long ones
maxlen = 700 

#Making all news of size maxlen defined above
X = pad_sequences(X, maxlen=maxlen)

#all news has 700 words (in numerical form now). If they had less words, they have been padded with 0
# 0 is not associated to any word, as mapping of words started from 1
# 0 will also be used later, if unknows word is encountered in test set
len(X[0])

# Adding 1 because of reserved 0 index
# Embedding Layer creates one more vector for "UNKNOWN" words, or padded words (0s). This Vector is filled with zeros.
# Thus our vocab size inceeases by 1
vocab_size = len(tokenizer.word_index) + 1

# Function to create weight matrix from word2vec gensim model
def get_weight_matrix(model, vocab):
    # total vocabulary size plus 0 for unknown words
    vocab_size = len(vocab) + 1
    # define weight matrix dimensions with all 0
    weight_matrix = np.zeros((vocab_size, EMBEDDING_DIM))
    # step vocab, store vectors using the Tokenizer's integer mapping
    for word, i in vocab.items():
        weight_matrix[i] = model[word]
    return weight_matrix

#Getting embedding vectors from word2vec and usings it as weights of non-trainable keras embedding layer
embedding_vectors = get_weight_matrix(w2v_model, word_index)
pd.DataFrame(embedding_vectors).to_csv('/content/real.csv')

len(embedding_vectors[0])

!pip install wandb
!pip install shap

# Commented out IPython magic to ensure Python compatibility.
# import the required modules
# improves output by ignoring warnings
import warnings
warnings.filterwarnings('ignore')
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, plot_confusion_matrix
from pprint import pprint
import matplotlib.pyplot as plt
# %matplotlib inline
# Bayesian optimization
from hyperopt import hp,fmin,tpe,STATUS_OK,Trials
#import wandb

positive = pd.read_csv("/content/real.csv")
label = [1 for i in range(positive.shape[0])]
positive['Label'] = label

negative = pd.read_csv("/content/fake.csv")
label = [0 for i in range(negative.shape[0])]
negative['Label'] = label

positive.head()

negative.head()

dataset = positive.append(negative , ignore_index=True)
dataset.head()
dataset.shape[0]

pip install xgboost

pip install --upgrade xgboost

from numpy import loadtxt
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

#load data
import numpy as np
embeddings = dataset.drop(['Unnamed: 0','Label'],axis=1)
label = dataset['Label']
x = embeddings.to_numpy()
y = label.to_numpy()

# split data into train and test sets.  #three splits- there will be train data-training, test data-hyperparameter tuning, validation data- accuracy testing
def split_dataset(X, Y, train_ratio, test_ratio, validation_ratio):
    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=1 - train_ratio)
    x_val, x_test, y_val, y_test = train_test_split(x_test, y_test, test_size=test_ratio/(test_ratio + validation_ratio))
    return x_train, y_train, x_test, y_test, x_val, y_val

train_ratio = 0.70
validation_ratio = 0.15
test_ratio = 0.15

# train is now 70% of the entire data set
# test is now 15% of the initial data set
# validation is now 15% of the initial data set
x_train, y_train, x_test, y_test, x_val, y_val=split_dataset(x, y, train_ratio, test_ratio, validation_ratio)

# fit model no training data
xgbclassifier = XGBClassifier()
xgbclassifier.fit(x_train, y_train)

# predicted values from the model
y_pred=xgbclassifier.predict(x_test)
y_probas=xgbclassifier.predict_proba(x_test)

#wandb.log({'accuracy': accuracy_score(y_test, y_pred)})

# accuracy prediction
accuracy = accuracy_score(y_test, y_pred)
print("BASE MODEL")
print("Accuracy: %.2f%%" % (accuracy * 100.0))


# classification report
print("Classification report:\n")
print(classification_report(y_test, y_pred))

# confusion matrix
conf=confusion_matrix(y_test, y_pred)
print("Confusion matrix:\n", conf)

# confusion matrix plot
print("Confusion matrix plot:\n")
plot_confusion_matrix(xgbclassifier, x_test, y_test) 
plt.show()

import shap
explainer = shap.TreeExplainer(xgbclassifier)
shap_values = explainer.shap_values(x_test)

shap_values

l=shap_values[0]
len(l)

shap.summary_plot(shap_values, x_test, plot_type="bar")

len(xgbclassifier.feature_importances_)

# parameters cuurently used
print('Parameters currently in use:')
pprint(xgbclassifier.get_params())

# Hyperparameter tuning using GridsearchCV
# Setting range of parameters
#Learning rate shrinks the weights to make the boosting process more conservative
learning_rate = [0.0001,0.001, 0.01, 0.1, 1]
# Number of trees in xgboost decision tree
n_estimators = [1000, 2000, 3000, 4000, 5000]
# Number of features to consider at every split
max_features = ['auto', 'sqrt']
# Maximum number of levels in tree
max_depth = [1, 2, 3, 4, 5]
#  Percentage of columns to be randomly samples for each tree.
colsample_bytree = [i/10.0 for i in range(3,10)]
#  Gamma specifies the minimum loss reduction required to make a split
gamma = [i/10.0 for i in range(0,5)]
#  reg_alpha provides l1 regularization to the weight, higher values result in more conservative models
reg_alpha = [1e-5, 1e-2, 0.1, 1, 10, 100]
# reg_lambda provides l2 regularization to the weight, higher values result in more conservative models
reg_lambda = [1e-5, 1e-2, 0.1, 1, 10, 100]
# Create the random grid
param = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'colsample_bytree': colsample_bytree,
               'gamma': gamma,
               'reg_alpha': reg_alpha,
               'reg_lambda': reg_lambda,
               'learning_rate':learning_rate
         }

print("\nGRID SEARCH MODEL")
print('Range of parameters used for hyperparameter tuning:')
pprint(param)
# implemented grid search 
classifier_grid=GridSearchCV(estimator=xgbclassifier, param_grid=param, cv = 3, verbose=2, n_jobs = 4)
# fit the training data
classifier_grid.fit(x_train,y_train)

# Best hyperparameter values
print('Best parameter values:')
print(classifier_grid.best_params_)

# predicted values from the grid search model
cl_g=classifier_grid.best_estimator_
pred=cl_g.predict(x_test)
y_probas = cl_g.predict_proba(x_test)

# accuracy prediction for grid search model
accuracy = accuracy_score(y_test, pred)
print("Accuracy: %.2f%%" % (accuracy * 100.0))

#classification report
print("Classification report:\n")
print(classification_report(y_test, y_pred))

# confusion matrix
conf_g=confusion_matrix(y_test, pred)
print("Confusion matrix\n", conf_g)

# confusion matrix plot
print("Confusion matrix plot:\n")
plot_confusion_matrix(cl_g, x_test, y_test)  
plt.show()

explainer = shap.TreeExplainer(cl_g)
shap_values = explainer.shap_values(x_test)
shap.summary_plot(shap_values, x_test, plot_type="bar")

# Random search implementation
# Module for hyperparameter tuning
# Hyperparameter tuning using RandomizedsearchCV
# Setting range of parameters
#Learning rate shrinks the weights to make the boosting process more conservative
learning_rate = [0.0001,0.001, 0.01, 0.1, 1]
# Number of trees in xgboost decision tree
n_estimators = [1000, 2000, 3000, 4000, 5000]
# Number of features to consider at every split
max_features = ['auto', 'sqrt']
# Maximum number of levels in tree
max_depth = [1, 2, 3, 4, 5]
#  Percentage of columns to be randomly samples for each tree.
colsample_bytree = [i/10.0 for i in range(3,10)]
#  Gamma specifies the minimum loss reduction required to make a split
gamma = [i/10.0 for i in range(0,5)]
#  reg_alpha provides l1 regularization to the weight, higher values result in more conservative models
reg_alpha = [1e-5, 1e-2, 0.1, 1, 10, 100]
# reg_lambda provides l2 regularization to the weight, higher values result in more conservative models
reg_lambda = [1e-5, 1e-2, 0.1, 1, 10, 100]
# Create the random grid
param = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'colsample_bytree': colsample_bytree,
               'gamma': gamma,
               'reg_alpha': reg_alpha,
               'reg_lambda': reg_lambda,
               'learning_rate':learning_rate
         }
print("\nRANDOM SEARCH MODEL")
print('Range of parameters used for hyperparameter tuning:')
pprint(param)

# implemented random search
classifier_random=RandomizedSearchCV(estimator = xgbclassifier, param_distributions = param, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)

# fit the training data in the randomized model
classifier_random.fit(x_train, y_train)

# Best hyperparameter values
print('Best parameter values:')
print(classifier_random.best_params_)

# predicted values from the random search model using best parameters
cl_r=classifier_random.best_estimator_
pred=cl_r.predict(x_test)
y_probas = cl_r.predict_proba(x_test)

# accuracy prediction for random search model
accuracy = accuracy_score(y_test, pred)
print("Accuracy: %.2f%%" % (accuracy * 100.0))

#classification report
print("Classification report:\n")
print(classification_report(y_test, y_pred))

# confusion matrix
conf_r=confusion_matrix(y_test, pred)
print("Confusion matrix\n", conf_r)

# confusion matrix plot
print("Confusion matrix plot:\n")
plot_confusion_matrix(cl_r, x_test, y_test)  
plt.show()

explainer = shap.TreeExplainer(cl_r)
shap_values = explainer.shap_values(x_test)
shap.summary_plot(shap_values, x_test, plot_type="bar", feature_names=feature_name)

# Bayesian optimization
print("\nBAYESIAN OPTIMIZATION")
from sklearn.model_selection import cross_val_score
from hyperopt import tpe, STATUS_OK, Trials, hp, fmin, STATUS_OK, space_eval
space = {
    'learning_rate': hp.choice('learning_rate', [0.0001,0.001, 0.01, 0.1, 1]),
    'max_depth' : hp.choice('max_depth', range(3,21,3)),
    'gamma' : hp.choice('gamma', [i/10.0 for i in range(0,5)]),
    'colsample_bytree' : hp.choice('colsample_bytree', [i/10.0 for i in range(3,10)]),
    'reg_alpha' : hp.choice('reg_alpha', [1e-5, 1e-2, 0.1, 1, 10, 100]),
    'reg_lambda' : hp.choice('reg_lambda', [1e-5, 1e-2, 0.1, 1, 10, 100])
}

def objective(space):
   model = XGBClassifier(seed=0, **space)
   #5 times cross validation fives 5 accuracies=>mean of these accuracies will be considered
   accuracy = cross_val_score(model, x_train, y_train, cv = 5).mean()
   # We aim to maximize accuracy, therefore we return it as a negative value
   return {'loss': -accuracy, 'status': STATUS_OK }

# Trials to track progress
bayes_trials = Trials()
# Optimize
best = fmin(fn = objective, space = space, algo = tpe.suggest, max_evals = 48, trials = bayes_trials)
# Print the index of the best parameters
print(best)
# Print the values of the best parameters
print(space_eval(space, best))
# Train model using the best parameters
xgboost_bayesian = XGBClassifier(seed=0,
                           colsample_bytree=0.4,
                           gamma=0.2,
                           learning_rate=1,
                           max_depth=12,
                           reg_alpha=1e-05,
                           reg_lambda=1
                           ).fit(x_train,y_train)

pred_b = xgboost_bayesian.predict(x_test)

# accuracy prediction
accuracy = accuracy_score(y_test, pred_b)
print("Accuracy: %.2f%%" % (accuracy * 100.0))

# classification report
print("Classification report:\n")
print(classification_report(y_test, pred_b))

# confusion matrix
conf=confusion_matrix(y_test, pred_b)
print("Confusion matrix:\n", conf)

# confusion matrix plot
print("Confusion matrix plot:\n")
plot_confusion_matrix(xgboost_bayesian, x_test, y_test)  
plt.show()

# Bayesian search
import pandas as pd

pos = pd.read_csv("/content/real.csv", skiprows=1, sep=" ", index_col=0, header=None)
neg = pd.read_csv("/content/fake.csv", skiprows=1, sep=" ", index_col=0, header=None)
df = pd.concat([pos, neg])
#df.iloc[:,:0]
# this converts the index (ie k-mers) into a list
p=pos.index.tolist()
n=neg.index.tolist()
posd=pos.index
negd=neg.index
whole=posd.append(negd)
kmer=list(whole)

# use countvectorizer
from sklearn.feature_extraction.text import CountVectorizer
import matplotlib.pyplot as plt
import numpy as np

# Load the text data
corpus=kmer

vectorizer = CountVectorizer()
docs       = vectorizer.fit_transform(corpus)
features   = vectorizer.get_feature_names()

# step 2
feature_name=np.array(features)

# step 3
sorted_idx = xgboost_bayesian.feature_importances_.argsort()#[::-1]
#feature_name = [feature_name[i] for i in sorted_idx]
n_top_features=50
plt.figure(figsize=(8,10))
plt.barh(feature_name[sorted_idx][:n_top_features ],xgbclassifier.feature_importances_[sorted_idx][:n_top_features ])
plt.xlabel("XGBoost feature Importance ")

explainer = shap.TreeExplainer(cl_r)
shap_values = explainer.shap_values(x_test)
shap.summary_plot(shap_values, x_test, plot_type="bar", feature_names=feature_name)

# Weights and biases
import wandb
wandb.init(project="XGBoost classifier", name="XGBoost-base model")
# Feature importance
wandb.sklearn.plot_feature_importances(xgbclassifier)
# metrics summary
wandb.sklearn.plot_summary_metrics(xgbclassifier, x_train, y_train, x_test, y_test)
# precision recall
wandb.sklearn.plot_precision_recall(y_test, y_probas, labels=None)
# ROC curve
wandb.sklearn.plot_roc(y_test, y_probas, labels=None)
# Learning curve
wandb.sklearn.plot_learning_curve(xgbclassifier, x_train, y_train)
# class proportions
wandb.sklearn.plot_class_proportions(y_train, y_test, labels=None)
# calibration curve
wandb.sklearn.plot_calibration_curve(xgbclassifier, x, y, 'XGBoost- Base model')
#confusion matrix
wandb.sklearn.plot_confusion_matrix(y_test, y_pred, labels=None)

import wandb
wandb.init(project="XGBoost classifier", name="XGBoost-gridsearch model")
# Feature importance
wandb.sklearn.plot_feature_importances(cl_g)
# metrics summary
wandb.sklearn.plot_summary_metrics(cl_g, x_train, y_train, x_test, y_test)
# precision recall
wandb.sklearn.plot_precision_recall(y_test, y_probas, labels=None)
# ROC curve
wandb.sklearn.plot_roc(y_test, y_probas, labels=None)
# Learning curve
wandb.sklearn.plot_learning_curve(cl_g, x_train, y_train)
# class proportions
wandb.sklearn.plot_class_proportions(y_train, y_test, labels=None)
# calibration curve
wandb.sklearn.plot_calibration_curve(cl_g, x, y, 'XGBoost-Grid search model')
# confusion matrix
wandb.sklearn.plot_confusion_matrix(y_test, y_pred, labels=None)

wandb.init(project="XGBoost classifier", name="XGBoost-random search model")
# Feature importance
wandb.sklearn.plot_feature_importances(cl_r)
# metrics summary
wandb.sklearn.plot_summary_metrics(cl_r, x_train, y_train, x_test, y_test)
# precision recall
wandb.sklearn.plot_precision_recall(y_test, y_probas, labels=None)
# ROC curve
wandb.sklearn.plot_roc(y_test, y_probas, labels=None)
# Learning curve
wandb.sklearn.plot_learning_curve(cl_r, x_train, y_train)
# class proportions
wandb.sklearn.plot_class_proportions(y_train, y_test, labels=None)
# calibration curve
wandb.sklearn.plot_calibration_curve(cl_r, x, y, 'XGBoost-Random search model')
# confusion matrix
wandb.sklearn.plot_confusion_matrix(y_test, y_pred, labels=None)

wandb.init(project="XGBoost classifier", name="XGBoost-Bayesian optimization model")
# Feature importance
wandb.sklearn.plot_feature_importances(model=xgboost_bayesian, title="XGBoost-Bayesian model")
# metrics summary
wandb.sklearn.plot_summary_metrics(xgboost_bayesian, x_train, y_train, x_test, y_test)
# precision recall
wandb.sklearn.plot_precision_recall(y_test, y_probas, labels=None)
# ROC curve
wandb.sklearn.plot_roc(y_test, y_probas, labels=None)
# Learning curve
wandb.sklearn.plot_learning_curve(xgboost_bayesian, x_train, y_train)
# class proportions
wandb.sklearn.plot_class_proportions(y_train, y_test, labels=None)
# calibration curve
wandb.sklearn.plot_calibration_curve(xgboost_bayesian, x, y, 'XGBoost-Bayesian optimization model')
# confusion matrix
wandb.sklearn.plot_confusion_matrix(y_test, y_pred, labels=None)

# Exporting metrics from a project in to a CSV file
import pandas as pd 
import wandb
api = wandb.Api()
entity, project = "tyagilab", "XGBoost classifier"  # set to your entity and project 
runs = api.runs(entity + "/" + project) 

summary_list, config_list, name_list = [], [], []
for run in runs: 
    # .summary contains the output keys/values for metrics like accuracy.
    summary_list.append(run.summary._json_dict)

    # .config contains the hyperparameters.
    #  We remove special values that start with _.
    config_list.append(
        {k: v for k,v in run.config.items()
         if not k.startswith('_')})

    # .name is the human-readable name of the run.
    name_list.append(run.name)

runs_df = pd.DataFrame({
    "summary": summary_list,
    "config": config_list,
    "name": name_list
    })

runs_df.to_csv("project.csv")