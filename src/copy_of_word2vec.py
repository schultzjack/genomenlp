# -*- coding: utf-8 -*-
"""Copy of word2vec.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1D6u23umG-TNIFOB7YVAe-bzmr4nFuVda
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import nltk
import re
from wordcloud import WordCloud

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Embedding, LSTM, Conv1D, MaxPool1D
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score

news = pd.read_csv("/content/fake_or_real_news.csv")

news.head()
news.keys()

#Counting by Label
for key,count in news.label.value_counts().iteritems():
    print(f"{key}:\t{count}")
    
#Getting Total Rows
print(f"Total Records:\t{news.shape[0]}")

plt.figure(figsize=(8,5))
sns.countplot("label", data=news)
plt.show()

import nltk
nltk.download('stopwords')

#Word Cloud
texts = ''
for news_i in news.text.values:
    texts += f" {news_i}"
wordcloud = WordCloud(
    width = 3000,
    height = 2000,
    background_color = 'black',
    stopwords = set(nltk.corpus.stopwords.words("english"))).generate(texts)
fig = plt.figure(
    figsize = (40, 30),
    facecolor = 'k',
    edgecolor = 'k')
plt.imshow(wordcloud, interpolation = 'bilinear')
plt.axis('off')
plt.tight_layout(pad=0)
plt.show()
del texts

news.head()

#checking for rows with empty text like row:8970
[index for index,text in enumerate(news.text.values) if str(text).strip() == '']
#seems only one :)

#dropping the records
news = news.drop([106,710,
 806,
 919,
 940,
 1664,
 1736,
 1851,
 1883,
 1941,
 2244,
 2426,
 2576,
 2662,
 2788,
 2832,
 3073,
 3350,
 3511,
 3641,
 3642,
 4014,
 4142,
 4253,
 4713,
 4744,
 5017,
 5088,
 5213,
 5581,
 5639,
 5699,
 5772,
 6064,
 6175,
 6328]
, axis=0)

news.head(6329)

news.keys()

fake = news[news['label'] == 'FAKE']
fake.head()

real = news[news['label'] == 'REAL']
real.head()

# Adding class Information
real["class"] = 1
fake["class"] = 0

#Combining Title and Text
real["text"] = real["title"] + " " + real["text"]
fake["text"] = fake["title"] + " " + fake["text"]

# Subject is diffrent for real and fake thus dropping it
# Aldo dropping Date, title and Publication Info of real
real = real.drop(["Unnamed: 0","title","label"], axis=1)
fake = fake.drop(["Unnamed: 0","title","label"], axis=1)

#Embeddings for real data
data_1 = real
del real

nltk.download('stopwords')
nltk.download('punkt')

y = data_1["class"].values
#Converting X to format acceptable by gensim, removing annd punctuation stopwords in the process
X_1 = []
stop_words = set(nltk.corpus.stopwords.words("english"))
tokenizer = nltk.tokenize.RegexpTokenizer(r'\w+')
for par in data_1["text"].values:
    tmp = []
    sentences = nltk.sent_tokenize(par)
    for sent in sentences:
        sent = sent.lower()
        tokens = tokenizer.tokenize(sent)
        filtered_words = [w.strip() for w in tokens if w not in stop_words and len(w) > 1]
        tmp.extend(filtered_words)
    X_1.append(tmp)

del data_1

#Embeddings for real data
data_0 = fake
del fake

nltk.download('stopwords')
nltk.download('punkt')

y = data_0["class"].values
#Converting X to format acceptable by gensim, removing annd punctuation stopwords in the process
X_0 = []
stop_words = set(nltk.corpus.stopwords.words("english"))
tokenizer = nltk.tokenize.RegexpTokenizer(r'\w+')
for par in data_0["text"].values:
    tmp = []
    sentences = nltk.sent_tokenize(par)
    for sent in sentences:
        sent = sent.lower()
        tokens = tokenizer.tokenize(sent)
        filtered_words = [w.strip() for w in tokens if w not in stop_words and len(w) > 1]
        tmp.extend(filtered_words)
    X_0.append(tmp)

del data_0

import gensim

#Dimension of vectors we are generating
EMBEDDING_DIM = 100

#Creating Word Vectors by Word2Vec Method (takes time...)
w2v_model_0 = gensim.models.Word2Vec(sentences=X_0, size=EMBEDDING_DIM, window=5, min_count=1)
w2v_model_1 = gensim.models.Word2Vec(sentences=X_0, size=EMBEDDING_DIM, window=5, min_count=1)

len(w2v_model_0.wv.vocab)

w2v_model_0["corona"]

w2v_model_0.wv.most_similar("iran")

w2v_model_0.wv.most_similar("fbi")

w2v_model_0.wv.most_similar("facebook")

w2v_model_0.wv.most_similar("computer")

#Feeding US Presidents
w2v_model_0.wv.most_similar(positive=["trump","obama", "clinton"])
#First was Bush

# Tokenizing Text -> Repsesenting each word by a number
# Mapping of orginal word to number is preserved in word_index property of tokenizer

#Tokenized applies basic processing like changing it yo lower case, explicitely setting that as False
tokenizer = Tokenizer()
tokenizer.fit_on_texts(X_0)

X_0 = tokenizer.texts_to_sequences(X_0)

# lets check the first 10 words of first news
#every word has been represented with a number
X_0[0][:10]

#Lets check few word to numerical replesentation
#Mapping is preserved in dictionary -> word_index property of instance
word_index = tokenizer.word_index
for word, num in word_index.items():
    print(f"{word} -> {num}")
    if num == 10:
        break

# For determining size of input...

# Making histogram for no of words in news shows that most news article are under 700 words.
# Lets keep each news small and truncate all news to 700 while tokenizing
plt.hist([len(x) for x in X_0], bins=500)
plt.show()

# Its heavily skewed. There are news with 5000 words? Lets truncate these outliers :)

nos = np.array([len(x) for x in X_0])
len(nos[nos  < 700])

#Lets keep all news to 700, add padding to news with less than 700 words and truncating long ones
maxlen = 700 

#Making all news of size maxlen defined above
X = pad_sequences(X_0, maxlen=maxlen)

#all news has 700 words (in numerical form now). If they had less words, they have been padded with 0
# 0 is not associated to any word, as mapping of words started from 1
# 0 will also be used later, if unknows word is encountered in test set
len(X_0[0])

# Adding 1 because of reserved 0 index
# Embedding Layer creates one more vector for "UNKNOWN" words, or padded words (0s). This Vector is filled with zeros.
# Thus our vocab size inceeases by 1
vocab_size = len(tokenizer.word_index) + 1

# Function to create weight matrix from word2vec gensim model
def get_weight_matrix(model, vocab):
    # total vocabulary size plus 0 for unknown words
    vocab_size = len(vocab) + 1
    # define weight matrix dimensions with all 0
    weight_matrix = np.zeros((vocab_size, EMBEDDING_DIM))
    # step vocab, store vectors using the Tokenizer's integer mapping
    for word, i in vocab.items():
        weight_matrix[i] = model[word]
    return weight_matrix

#Getting embedding vectors from word2vec and usings it as weights of non-trainable keras embedding layer
embedding_vectors_0 = get_weight_matrix(w2v_model_0, word_index)
pd.DataFrame(embedding_vectors_0).to_csv('/content/fake.csv')

embedding_vectors_1 = get_weight_matrix(w2v_model_1, word_index)
pd.DataFrame(embedding_vectors_1).to_csv('/content/real.csv')

len(embedding_vectors_0[0])

positive = pd.read_csv("/content/real.csv")
label = [1 for i in range(positive.shape[0])]
positive['Label'] = label

negative = pd.read_csv("/content/fake.csv")
label = [0 for i in range(negative.shape[0])]
negative['Label'] = label

positive.head()

negative.head()

dataset = positive.append(negative , ignore_index=True)
dataset.head()
dataset.shape[0]

pip install xgboost

pip install --upgrade xgboost

from numpy import loadtxt
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

#load data
import numpy as np
embeddings = dataset.drop(['Unnamed: 0','Label'],axis=1)
label = dataset['Label']
X = embeddings.to_numpy()
y = label.to_numpy()

# split data into train and test sets
seed = 7
test_size = 0.33
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=seed)

# fit model no training data
model = XGBClassifier()
model.fit(X_train, y_train)

print(model)

# make predictions for test data
y_pred = model.predict(X_test)
predictions = [round(value) for value in y_pred]

# evaluate predictions
accuracy = accuracy_score(y_test, predictions)
print("Accuracy: %.2f%%" % (accuracy * 100.0))