# -*- coding: utf-8 -*-
"""RF.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WPFurDRYx9p6OvYM1hAmK2Ald3DE4jKD

Hi Navya, thanks for this, the flow of logic is clear and I can see what you are doing. I have four action items which I list below, please have a look. Most are related to conventions and general best practices in programming:

1.  Can you upload the code to `git` please? This is so all the code is in a central location for us (a) to review together (b) avoid redundancy and (c) track progress. It is perfectly fine to put rough versions here also.

On your terminal:

Navigate to a directory of your choice in the example below i use `~/Desktop`

```
cd ~/Desktop
```

Clone the git repository. If this step fails, let me know

```
git clone 'https://gitlab.com/tyagilab/ziran.git'
```

You can inspect the repository to see what is inside

```
cd ziran
ls
```

Here is where i put the code

```
cd src
```

Now paste and add your RF file here. Please add it as a .py file and not a .ipynb file. (you can of course keep the original .ipynb file  to continue to experiment with)

```
git add rf.py
```

Add an informative but concise commit message. Here is an example:

```
git commit -m 'Classify embeddings with Random Forest'
```

Now upload this to `git`

```
git push
```

2.  It is always good practice to include a small extract of input data (and if possible, a link to it)

Even though we know how the embedding file looks like, it is always useful to have a small example on hand for reference. 
- The main reason for this is so that it speeds up development as we can change and quickly retest code on the sample file. 
- Also, new users can easily see what to put in (and developers who work on many different projects can quickly refresh their memory on this specific one).

In most cases, we take a small extract of the file (especially if the data is big [>10MB] !) and commit it to the `data` directory of `ziran`. This is the "usual" method.

3.  File parsing

I see that you are manually parsing files with `open()` and doing `.split` on each line. There is nothing incorrect here and your code is fine ðŸ™‚. However, we should consider using existing file parsers if already available. This (a) makes life easier as we dont need to reinvent the wheel and (b) minimises errors as we use a package that has been used and tested. I recommend rewriting the first block to use `pandas` instead. It is compatible with `scikit-learn` so there should be no problems. If `pandas` is not already installed, you can install it in colaboratory with:

```
!pip install pandas
```

or in terminal with:

```
pip3 install pandas
```

4.  [OPTIONAL] Modular functions. *You do not have to do this right now, but we will be doing this later on for the final product.*

Same as (3), there is nothing incorrect about your code, this is more to design philosophy.

The way you wrote your code is in a single block of text. What the end result will look like should be more to a collection of independent functions each doing a specific thing, with a controller function sitting on top. See for example: [generate_synthetic.py](https://gitlab.com/tyagilab/ziran/-/blob/main/src/generate_synthetic.py) imports multiple functions from [utils.py](https://gitlab.com/tyagilab/ziran/-/blob/main/src/utils.py) and performs its functions accordingly.

- The main reason for this design philosophy is that it is easier to debug, as we split every component into modular parts which we can run and test individually.
- It also allows us to reuse and recycle components in different scripts.
"""

# RF classification model
import numpy
# data
f_pos=open('Positive.w2v','r')
f_neg=open('Negative.w2v','r')
file_p=f_pos.read()
file_n=f_neg.read()
lis_p=[x.split() for x in file_p.split('\n')[1:-1]]
lis_n=[x.split() for x in file_n.split('\n')[1:-1]]
list_p=[[float(x) for x in y[1:]] for y in lis_p]
list_n=[[float(x) for x in y[1:]] for y in lis_n]
l_pos=[x+[0] for x in list_p]
l_neg=[x+[0] for x in list_n]
l_whole = l_pos+l_neg
dataset = numpy.array([numpy.array(x) for x in l_whole])

# split data into X and Y
X = dataset[:,:-1]
Y = dataset[:,-1]

# split the data into train and test sklearn
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.20)

# RF classifier model
from sklearn.ensemble import RandomForestClassifier
classifier=RandomForestClassifier(n_estimators=20, criterion="gini", random_state=1, max_depth=3)
classifier.fit(x_train, y_train)

# predict values from the model
y_pred=classifier.predict(x_test)

# accuracy prediction
from sklearn.metrics import accuracy_score
print(accuracy_score(y_test, y_pred))